#run the classifier, can change k
classif<-kNN(train,test,cg,k=5)
#run the classifier, can change k
library(class)
classif<-kNN(train,test,cg,k=5)
classif<-knn(train,test,cg,k=5)
#run the classifier, can change k
library(class)
classif<-knn(train,test,cg,k=5)
#view the classifier
classif
#looks at attriburtes
attributes(.Last.value)
library(kknn)
data(ionosphere)
ionosphere.learn <- ionosphere[1:200,]
library(kknn)
data(ionosphere)
ionosphere.learn <- ionosphere[1:200,]
ionosphere.valid <- ionosphere[-c(1:200),]
fit.kknn <- kknn(class ~ ., ionosphere.learn, ionosphere.valid)
table(ionosphere.valid$class, fit.kknn$fit)
(fit.train1 <- train.kknn(class ~ ., ionosphere.learn, kmax = 15,
kernel = c("triangular", "rectangular", "epanechnikov", "optimal"), distance = 1))
table(predict(fit.train1, ionosphere.valid), ionosphere.valid$class)
(fit.train2 <- train.kknn(class ~ ., ionosphere.learn, kmax = 15,
kernel = c("triangular", "rectangular", "epanechnikov", "optimal"), distance = 2))
table(predict(fit.train2, ionosphere.valid), ionosphere.valid$class)
library(kknn)
data(ionosphere)
ionosphere.learn <- ionosphere[1:200,]
ionosphere.valid <- ionosphere[-c(1:200),]
fit.kknn <- kknn(class ~ ., ionosphere.learn, ionosphere.valid)
table(ionosphere.valid$class, fit.kknn$fit)
(fit.train1 <- train.kknn(class ~ ., ionosphere.learn, kmax = 15,
kernel = c("triangular", "rectangular", "epanechnikov", "optimal"), distance = 1))
table(predict(fit.train1, ionosphere.valid), ionosphere.valid$class)
(fit.train2 <- train.kknn(class ~ ., ionosphere.learn, kmax = 15,
kernel = c("triangular", "rectangular", "epanechnikov", "optimal"), distance = 2))
table(predict(fit.train2, ionosphere.valid), ionosphere.valid$class)
data(iris)
m <- dim(iris)[1]
val <- sample(1:m, size = round(m/3), replace = FALSE,
prob = rep(1/m, m))
iris.learn <- iris[-val,]
iris.valid <- iris[val,]
iris.kknn <- kknn(Species~., iris.learn, iris.valid, distance = 1,
kernel = "triangular")
summary(iris.kknn)
fit <- fitted(iris.kknn)
table(iris.valid$Species, fit)
pcol <- as.character(as.numeric(iris.valid$Species))
pairs(iris.valid[1:4], pch = pcol, col = c("green3", "red")
[(iris.valid$Species != fit)+1])
data("iris")
iris.dist <- dist(iris[, -5])
iris.mds <- cmdscale(iris.dist)
# iris$Species is the 5th column
c.chars <- c("*", "o", "+")[as.integer(iris$Species)]
#fragment
set.seed(123)
sim.xy <- function(n, mean, sd) cbind(rnorm(n, mean[1], sd[1]),rnorm(n, mean[2],sd[2]))
# generate three clouds of points, well separated in the 2D plane
xy <- rbind(sim.xy(100, c(0,0), c(.2,.2)),sim.xy(100, c(2.5,0), c(.4,.2)),sim.xy(100, c(1.25,.5), c(.3,.2)))
xy[1,] <- c(0,2)     # convert 1st obs. to an outlying value
#
km3 <- kmeans(xy, 3) # ask for three clusters
plot(xy, col=km3$cluster)
cex=2.0
points(km3$centers, pch=3)
#
km4 <- kmeans(xy, 4) # ask for four clusters
cex=1.0
plot(xy, col=km4$cluster)
cex=2.0
points(km4$centers, pch=3)
data("iris")
iris.dist <- dist(iris[, -5])
iris.mds <- cmdscale(iris.dist)
# iris$Species is the 5th column
c.chars <- c("*", "o", "+")[as.integer(iris$Species)]
# iris$Species is the 5th column
# KMEANSRESULT is the variable you used in your kmeans lab assignment for the return variable.
KMEANSRESULT <- km4
a.cols <- rainbow(3)[KMEANSRESULT$cluster]
plot(iris.mds, col = a.cols, pch = c.chars, xlab = "X", ylab = "Y")
plot(iris$Species)
corr <- KMEANSRESULT$cluster == 4 - as.integer(iris$Species)
correct <- c("o", "x")[2 - corr]
plot(iris.mds, col = a.cols, pch = correct, xlab = "X", ylab = "Y")
data(Titanic)
Titanic_df <- as.data.frame(Titanic)
rpart_model <- rpart(Survived ~ ., data = Titanic_df, method = "class")
library(rpart)
library(party)
library(cluster)
data(Titanic)
Titanic_df <- as.data.frame(Titanic)
rpart_model <- rpart(Survived ~ ., data = Titanic_df, method = "class")
ctree_model <- ctree(Survived ~ ., data = Titanic_df)
numeric_data <- Titanic_df[, sapply(Titanic_df, is.numeric)]
scaled_data <- scale(numeric_data)
hclust_model <- hclust(dist(scaled_data))
plot(rpart_model)
text(rpart_model, use.n = TRUE)
plot(ctree_model)
plot(hclust_model)
groups <- cutree(hclust_model, k = 5)
# Cook's Distance example using mtcars
mtcars
head(mtcars)
str(mtcars)
# Create a linear model
model1 <- lm(mpg ~ cyl + wt, data = mtcars)
model1
# For getting help on Cook's distance function
help("cooks.distance")
# Plotting the model with Cook's Distance indicated
plot(model1, pch = 18, col = 'red', which = c(4))
# We can use the cooks.distance() function to identify the Cook's distance to each observation
cooks.distance(model1)
# Store Cook's Distance in a variable
CooksDistance <- cooks.distance(model1)
# Now we will round off the values to 5 decimal points so that it is easy to read
# we can use the round() function to round off values in R.
round(CooksDistance, 5)
sort(round(CooksDistance, 5))
# Outlier Detection using "Cooks Distance"
# Multivariate Regression using Cook's Distance
# Cook’s Distance is an estimate of the influence of a data point.
# Cook’s Distance is a summary of how much a regression model changes when the ith observation is removed from the data.
library(ISLR)
# Let's look at the baseball hitters dataset in ISLR package.
head(Hitters)
dim(Hitters)
is.na(Hitters) # check for the missing values.
# Outlier Detection using "Cooks Distance"
# Multivariate Regression using Cook's Distance
# Cook’s Distance is an estimate of the influence of a data point.
# Cook’s Distance is a summary of how much a regression model changes when the ith observation is removed from the data.
library(ISLR)
# Let's look at the baseball hitters dataset in ISLR package.
head(Hitters)
dim(Hitters)
is.na(Hitters) # check for the missing values.
# Now we will remove the NA (missing values) using the na.omit() function
HittersData <- na.omit(Hitters)
dim(HittersData) # checking the dimensions after removing the NAs.
glimpse(HittersData)
head(HittersData)
?glimpse
??glimpse
library(dplyr)
glimpse(HittersData)
head(HittersData)
# Now we will implement a multivariate regression model using all the features in the dataset to
# predict the salary of the Baseball player
SalaryPredictModel1 <- lm(Salary ~ ., data = HittersData)
summary(SalaryPredictModel1)
# Cook's Distance.
cooksD <- cooks.distance(SalaryPredictModel1)
influential <- cooksD[cooksD > (3 * mean(cooksD, na.rm = TRUE))]
influential
# We see that 18 players have a Cook’s Distance greater than 3x the mean.
# Let’s exclude these 18 players and rerun the model to see if we have a better fit in our regression model
names_of_influential <- names(influential) # checking the names of the influential/outlier players
names_of_influential
outliers <- HittersData[names_of_influential,]
Hitters_Without_Outliers <- HittersData %>% anti_join(outliers)
# Model 2: without the outliers
SalaryPredictModel2 <- lm(Salary ~ ., data = Hitters_Without_Outliers)
summary(SalaryPredictModel2)
# Normality Tests...
# Normal Distribution
# Read the documentation of the random distribution function
help("rnorm")
# Normality Tests...
# Normal Distribution
# Read the documentation of the random distribution function
help("rnorm")
set.seed(10)
data1 <- rnorm(50)
set.seed(30)
data2 <- rnorm(50)
# Shapiro-Wilk Normality Test
# Read the documentation of Shapiro-Wilk Normality Test
help("shapiro.test")
# As the test returns a p-value less than 0.05, we reject the null hypothesis
# and conclude that the population data is not normally distributed.
shapiro.test(data1)
hist(data1, col='green')
shapiro.test(data2)
# As the test returns a p-value greater than 0.05, we accept the null hypothesis
# and conclude that the population data is normally distributed.
hist(data2, col='steelblue') # the histogram shows that the curve is normally distributed in nature
# We set the seed to make the example reproducible make this example reproducible
set.seed(0)
# Poisson Distribution
help("rpois")
# create dataset of 100 random values generated from a Poisson distribution
data <- rpois(n=100, lambda=3)
# perform Shapiro-Wilk test for normality
shapiro.test(data)
# The p-value of the test turns out to be 0.0003393.
# Since this value is less than 0.05, we have sufficient evidence
# to say that the sample data does not come from a population that is normally distributed.
# This result shouldn't be surprising since we generated the sample data using the rpois() function,
# rpois() generates random values from a Poisson distribution.
hist(data, col='yellow')
# Anderson-Darling test for normality
help("ad.test")
# To conduct an Anderson-Darling Test in R, we can use the ad.test() function within the nortest library.
# The following examples shows how to conduct an Anderson-Darling Test to check
# whether or not a vector of 100 values follows a normal distribution:
# Make sure to install and load "nortest" library
# install.packages('nortest')
library(nortest)
# we use seed() function make this example reproducible
set.seed(1)
# defined vector of 100 values that are normally distributed
x <- rnorm(100, 0, 1)
# conduct Anderson-Darling Test to test for normality
ad.test(x)
# conduct Anderson-Darling Test to test for normality
ad.test(x)
# Anderson-Darling test for normality
help("ad.test")
# Normality Tests...
# Normal Distribution
# Read the documentation of the random distribution function
help("rnorm")
set.seed(10)
data1 <- rnorm(50)
set.seed(30)
data2 <- rnorm(50)
# Shapiro-Wilk Normality Test
# Read the documentation of Shapiro-Wilk Normality Test
help("shapiro.test")
# As the test returns a p-value less than 0.05, we reject the null hypothesis
# and conclude that the population data is not normally distributed.
shapiro.test(data1)
hist(data1, col='green')
shapiro.test(data2)
# As the test returns a p-value greater than 0.05, we accept the null hypothesis
# and conclude that the population data is normally distributed.
hist(data2, col='steelblue') # the histogram shows that the curve is normally distributed in nature
# We set the seed to make the example reproducible make this example reproducible
set.seed(0)
# Poisson Distribution
help("rpois")
# create dataset of 100 random values generated from a Poisson distribution
data <- rpois(n=100, lambda=3)
# perform Shapiro-Wilk test for normality
shapiro.test(data)
# The p-value of the test turns out to be 0.0003393.
# Since this value is less than 0.05, we have sufficient evidence
# to say that the sample data does not come from a population that is normally distributed.
# This result shouldn't be surprising since we generated the sample data using the rpois() function,
# rpois() generates random values from a Poisson distribution.
hist(data, col='yellow')
# Anderson-Darling test for normality
help("ad.test")
# To conduct an Anderson-Darling Test in R, we can use the ad.test() function within the nortest library.
# The following examples shows how to conduct an Anderson-Darling Test to check
# whether or not a vector of 100 values follows a normal distribution:
# Make sure to install and load "nortest" library
# install.packages('nortest')
library(nortest)
# we use seed() function make this example reproducible
set.seed(1)
# defined vector of 100 values that are normally distributed
x <- rnorm(100, 0, 1)
# conduct Anderson-Darling Test to test for normality
ad.test(x)
library(randomForest)
fitKF <- randomForest(Kyphosis ~ Age + Number + Start,   data=kyphosis)
print(fitKF) 	# view results
importance(fitKF) # importance of each predictor
#
fitSwiss <- randomForest(Fertility ~ Agriculture + Education + Catholic, data = swiss)
print(fitSwiss) # view results
importance(fitSwiss) # importance of each predictor
varImpPlot(fitSwiss)
plot(fitSwiss)
getTree(fitSwiss,1, labelVar=TRUE)
help(randomForest) # look at all the package contents and the randomForest method options
# look at rfcv - random forest cross-validation -
help(rfcv)
# other data....
data(imports85)
dist.au <- read.csv("http://rosetta.reltech.org/TC/v15/Mapping/data/dist-Aus.csv")
row.names(dist.au) <- dist.au[, 1]
dist.au <- read.csv("http://rosetta.reltech.org/TC/v15/Mapping/data/dist-Aus.csv")
row.names(dist.au) <- dist.au[, 1]
dist.au <- dist.au[, -1]
dist.au
fit <- cmdscale(dist.au, eig = TRUE, k = 2)
x <- fit$points[, 1]
y <- fit$points[, 2]
#
plot(x, y, pch = 19, xlim = range(x) + c(0, 600))
city.names <- c("Adelaide", "Alice Springs", "Brisbane", "Darwin", "Hobart",
"Melbourne", "Perth", "Sydney")
text(x, y, pos = 4, labels = city.names)
library(igraph)
g <- graph.full(nrow(dist.au))
V(g)$label <- city.names
layout <- layout.mds(g, dist = as.matrix(dist.au))
library(igraph)
g <- graph.full(nrow(dist.au))
V(g)$label <- city.names
layout <- layout_with_mds(g, dist = as.matrix(dist.au))
plot(g, layout = layout, vertex.size = 3)
library(graphics)
loc <- cmdscale(eurodist)
x <- loc[, 1]
y <- -loc[, 2] # reflect so North is at the top
## note asp = 1, to ensure Euclidean distances are represented correctly
plot(x, y, type = "n", xlab = "", ylab = "", asp = 1, axes = FALSE,
main = "cmdscale(eurodist)")
text(x, y, rownames(loc), cex = 0.6)
library(kknn)
data(iris)
m <- dim(iris)[1]
library(kknn)
data(iris)
m <- dim(iris)[1]
val <- sample(1:m, size = round(m/3), replace = FALSE, prob = rep(1/m, m))
iris.learn <- iris[-val,] 	# train
iris.valid <- iris[val,]	# test
iris.kknn <- train.kknn(Species~., iris.learn, distance = 1, kernel = c("triangular", "epanechnikov", "biweight", "triweight", "cos", "inv", "gaussian", "rank", "optimal") )
summary(iris.kknn)
table(predict(iris.kknn,iris.valid),iris.valid$Species)
head(iris.kknn$W)
head(iris.kknn$D)
head(iris.kknn$C)
head(iris.kknn$fitted.values)
View(iris)
View(iris.kknn)
summary(iris.knn)
summary(iris.kknn)
summary(iris.kknn)
head(iris.kknn$W)
iris.kknn <- train.kknn(Species ~ ., iris.learn, distance = 1, kernel = "optimal", kmax = 10)
head(iris.kknn$W)
str(iris.kknn)
head(iris.kknn$Sepal.Width)
table(predict(iris.kknn,iris.valid),iris.valid$Species)
head(iris.kknn$fitted.values[[1]])
iris.kknn <- train.kknn(Species~., iris.learn, distance = 1, kernel = c("triangular", "epanechnikov", "biweight", "triweight", "cos", "inv", "gaussian", "rank", "optimal") ,k=5)
iris.kknn <- train.kknn(Species~., iris.learn, distance = 1, kernel = c("triangular", "epanechnikov", "biweight", "triweight", "cos", "inv", "gaussian", "rank", "optimal") ,k=7)
iris.kknn <- train.kknn(Species~., iris.learn, distance = 1, kernel = c("triangular", "epanechnikov", "biweight", "triweight", "cos", "inv", "gaussian", "rank", "optimal"))
head(iris.kknn$W)
head(iris.kknn$D)
head(iris.kknn$C)
head(iris.kknn$fitted.values)
install.packages("MetaPCA")
library(MetaPCA)
#Spellman, 1998 Yeast cell cycle data set
#Consider each synchronization method as a separate data
data(Spellman)
install.packages("MetaPCA")
if (!requireNamespace("devtools", quietly = TRUE))
install.packages("devtools")
devtools::install_github("https://github.com/donkang75/MetaPCA")
library(MetaPCA)
#Spellman, 1998 Yeast cell cycle data set
#Consider each synchronization method as a separate data
data(Spellman)
pc <- list(alpha=prcomp(t(Spellman$alpha))$x, cdc15=prcomp(t(Spellman$cdc15))$x,
cdc28=prcomp(t(Spellman$cdc28))$x, elu=prcomp(t(Spellman$elu))$x)
#There are currently 4 meta-pca methods. Run either one of following four.
metaPC <- MetaPCA(Spellman, method="Eigen", doPreprocess=FALSE)
plot
metaPC <- MetaPCA(Spellman, method="RobustAngle", doPreprocess=FALSE)
install.packages("BiocManager")
metaPC <- MetaPCA(Spellman, method="RobustAngle", doPreprocess=FALSE)
metaPC <- MetaPCA(Spellman, method="SparseAngle", doPreprocess=FALSE)
#Comparing between usual pca and meta-pca
#The first lows are four data sets based on usual PCA, and
#the second rows are by MetaPCA
#We're looking for a cyclic pattern.
par(mfrow=c(2,4), cex=1, mar=c(0.2,0.2,0.2,0.2))
for(i in 1:4) {
plot(pc[[i]][,1], pc[[i]][,2], type="n", xlab="", ylab="", xaxt="n", yaxt="n")
text(pc[[i]][,1], pc[[i]][,2], 1:nrow(pc[[i]]), cex=1.5)
lines(pc[[i]][,1], pc[[i]][,2])
}
for(i in 1:4) {
plot(metaPC$x[[i]]$coord[,1], metaPC$x[[i]]$coord[,2], type="n", xlab="", ylab="", xaxt="n", yaxt="n")
text(metaPC$x[[i]]$coord[,1], metaPC$x[[i]]$coord[,2], 1:nrow(metaPC$x[[i]]$coord), cex=1.5)
lines(metaPC$x[[i]]$coord[,1], metaPC$x[[i]]$coord[,2])
}
#There are currently 4 meta-pca methods. Run either one of following four.
metaPC <- MetaPCA(Spellman, method="Eigen", doPreprocess=FALSE)
plot
#There are currently 4 meta-pca methods. Run either one of following four.
metaPC <- MetaPCA(Spellman, method="Eigen", doPreprocess=FALSE)
metaPC <- MetaPCA(Spellman, method="RobustAngle", doPreprocess=FALSE)
metaPC <- MetaPCA(Spellman, method="SparseAngle", doPreprocess=FALSE)
#Comparing between usual pca and meta-pca
#The first lows are four data sets based on usual PCA, and
#the second rows are by MetaPCA
#We're looking for a cyclic pattern.
par(mfrow=c(2,4), cex=1, mar=c(0.2,0.2,0.2,0.2))
for(i in 1:4) {
plot(pc[[i]][,1], pc[[i]][,2], type="n", xlab="", ylab="", xaxt="n", yaxt="n")
text(pc[[i]][,1], pc[[i]][,2], 1:nrow(pc[[i]]), cex=1.5)
lines(pc[[i]][,1], pc[[i]][,2])
}
for(i in 1:4) {
plot(metaPC$x[[i]]$coord[,1], metaPC$x[[i]]$coord[,2], type="n", xlab="", ylab="", xaxt="n", yaxt="n")
text(metaPC$x[[i]]$coord[,1], metaPC$x[[i]]$coord[,2], 1:nrow(metaPC$x[[i]]$coord), cex=1.5)
lines(metaPC$x[[i]]$coord[,1], metaPC$x[[i]]$coord[,2])
}
#There are currently 4 meta-pca methods. Run either one of following four.
metaPC <- MetaPCA(Spellman, method="Eigen", doPreprocess=FALSE)
metaPC <- MetaPCA(Spellman, method="RobustAngle", doPreprocess=FALSE)
if (!require("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install(version = "3.18")
library(MetaPCA)
#Spellman, 1998 Yeast cell cycle data set
#Consider each synchronization method as a separate data
data(Spellman)
pc <- list(alpha=prcomp(t(Spellman$alpha))$x, cdc15=prcomp(t(Spellman$cdc15))$x,
cdc28=prcomp(t(Spellman$cdc28))$x, elu=prcomp(t(Spellman$elu))$x)
#There are currently 4 meta-pca methods. Run either one of following four.
metaPC <- MetaPCA(Spellman, method="Eigen", doPreprocess=FALSE)
metaPC <- MetaPCA(Spellman, method="RobustAngle", doPreprocess=FALSE)
#Comparing between usual pca and meta-pca
#The first lows are four data sets based on usual PCA, and
#the second rows are by MetaPCA
#We're looking for a cyclic pattern.
par(mfrow=c(2,4), cex=1, mar=c(0.2,0.2,0.2,0.2))
for(i in 1:4) {
plot(pc[[i]][,1], pc[[i]][,2], type="n", xlab="", ylab="", xaxt="n", yaxt="n")
text(pc[[i]][,1], pc[[i]][,2], 1:nrow(pc[[i]]), cex=1.5)
lines(pc[[i]][,1], pc[[i]][,2])
}
for(i in 1:4) {
plot(metaPC$x[[i]]$coord[,1], metaPC$x[[i]]$coord[,2], type="n", xlab="", ylab="", xaxt="n", yaxt="n")
text(metaPC$x[[i]]$coord[,1], metaPC$x[[i]]$coord[,2], 1:nrow(metaPC$x[[i]]$coord), cex=1.5)
lines(metaPC$x[[i]]$coord[,1], metaPC$x[[i]]$coord[,2])
}
metaPC <- MetaPCA(Spellman, method="SparseAngle", doPreprocess=FALSE)
library(iter)
metaPC <- MetaPCA(Spellman, method="SparseAngle", doPreprocess=FALSE)
library(iterators)
metaPC <- MetaPCA(Spellman, method="SparseAngle", doPreprocess=FALSE)
#Comparing between usual pca and meta-pca
#The first lows are four data sets based on usual PCA, and
#the second rows are by MetaPCA
#We're looking for a cyclic pattern.
par(mfrow=c(2,4), cex=1, mar=c(0.2,0.2,0.2,0.2))
for(i in 1:4) {
plot(pc[[i]][,1], pc[[i]][,2], type="n", xlab="", ylab="", xaxt="n", yaxt="n")
text(pc[[i]][,1], pc[[i]][,2], 1:nrow(pc[[i]]), cex=1.5)
lines(pc[[i]][,1], pc[[i]][,2])
}
library(iterators)
metaPC <- MetaPCA(Spellman, method="SparseAngle", doPreprocess=FALSE)
library(dr)
data(ais)
# default fitting method is "sir"
s0 <- dr(LBM~log(SSF)+log(Wt)+log(Hg)+log(Ht)+log(WCC)+log(RCC)+
log(Hc)+log(Ferr),data=ais)
# Refit, using a different function for slicing to agree with arc.
summary(s1 <- update(s0,slice.function=dr.slices.arc))
# Refit again, using save, with 10 slices; the default is max(8,ncol+3)
summary(s2<-update(s1,nslices=10,method="save"))
# Refit, using phdres.  Tests are different for phd, and not
# Fit using phdres; output is similar for phdy, but tests are not justifiable.
summary(s3<- update(s1,method="phdres"))
# fit using ire:
summary(s4 <- update(s1,method="ire"))
# fit using Sex as a grouping variable.
s5 <- update(s4,group=~Sex)
#4 prostate cancer data which have three classes: normal, primary, metastasis
data(prostate)
#There are currently 4 meta-pca methods. Run either one of following four.
metaPC <- MetaPCA(prostate, method="Eigen", doPreprocess=FALSE, .scale=TRUE)
metaPC <- MetaPCA(prostate, method="Angle", doPreprocess=FALSE)
metaPC <- MetaPCA(prostate, method="RobustAngle", doPreprocess=FALSE)
metaPC <- MetaPCA(prostate, method="SparseAngle", doPreprocess=FALSE,iter=100)
#4 prostate cancer data which have three classes: normal, primary, metastasis
data(prostate)
#There are currently 4 meta-pca methods. Run either one of following four.
metaPC <- MetaPCA(prostate, method="Eigen", doPreprocess=FALSE, .scale=TRUE)
metaPC <- MetaPCA(prostate, method="Angle", doPreprocess=FALSE)
# I got this bug " With R version 3.5 or greater, install Bioconductor packages using BiocManager; see https://bioconductor.org/install"
# I already install Bioconductor base on the intruction shows in webside
metaPC <- MetaPCA(prostate, method="RobustAngle", doPreprocess=FALSE)
metaPC <- MetaPCA(prostate, method="SparseAngle", doPreprocess=FALSE,iter=100)
#Plotting 4 data in the same space!
coord <- foreach(dd=iter(metaPC$x), .combine=rbind) %do% dd$coord
#metaPC <- MetaPCA(prostate, method="SparseAngle", doPreprocess=FALSE,iter=100)
metaPC <- MetaPCA(prostate, method="SparseAngle", doPreprocess=FALSE)
#Plotting 4 data in the same space!
coord <- foreach(dd=iter(metaPC$x), .combine=rbind) %do% dd$coord
PlotPC2D(coord[,1:2], drawEllipse=F, dataset.name="Prostate", .class.order=c("Metastasis","Primary","Normal"),
.class.color=c('red','#838383','blue'), .annotation=T, newPlot=T,
.class2=rep(names(metaPC$x), times=sapply(metaPC$x,function(x)nrow(x$coord))),
.class2.order=names(metaPC$x), .points.size=1)
#In the case of "SparseAngle" method, the top contributing genes for all studies can be determined
#For instance, top 20 genes in 1st PC and their coefficients
metaPC$v[order(abs(metaPC$v[,1]), decreasing=TRUE),1][1:20]
install.packages("EDR")
library(EDR)
install.packages("EDR")
install.packages("EDR")
install.packages("EDR")
