# Select three columns from the cleaned data without outliers
selected_data <- clean.df %>% select(SALE.PRICE, GROSS.SQUARE.FEET, LAND.SQUARE.FEET)
# Randomly sample 5000 records from the selected columns
sampled_data <- sample_n(selected_data, size = 5000)
# Partition the data into training and testing sets with a split of 80/20
partition_indexes <- createDataPartition(y = sampled_data$SALE.PRICE, p = 0.8, list = FALSE)
training_set <- sampled_data[partition_indexes, ]
testing_set <- sampled_data[-partition_indexes, ]
# Train a Random Forest model on the training data
random_forest_model <- randomForest(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET, data = training_set, ntree = 500)
# Make predictions on the testing data using the trained model
predicted_values <- predict(random_forest_model, testing_set)
# Plot the actual vs predicted Sale Prices
plot(testing_set$SALE.PRICE, predicted_values, xlab = "Actual Sale Price", ylab = "Predicted Sale Price", main = "Comparison of Actual and Predicted Sale Prices")
set.seed(123)
selected_data <- clean.df %>% select(SALE.PRICE, GROSS.SQUARE.FEET, LAND.SQUARE.FEET)
sampled_data <- sample_n(selected_data, size = 5000)
partition_indexes <- createDataPartition(y = sampled_data$SALE.PRICE, p = 0.8, list = FALSE)
training_set <- sampled_data[partition_indexes, ]
testing_set <- sampled_data[-partition_indexes, ]
random_forest_model <- randomForest(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET,
data = training_set, ntree = 500)
predicted_values <- predict(random_forest_model, testing_set)
plot(testing_set$SALE.PRICE, predicted_values,
xlab = "Actual Sale Price",
ylab = "Predicted Sale Price",
main = "Comparison of Actual and Predicted Sale Prices")
set.seed(1024)
selected_data <- clean.df %>% select(SALE.PRICE, GROSS.SQUARE.FEET, LAND.SQUARE.FEET)
sampled_data <- sample_n(selected_data, size = 5000)
partition_indexes <- createDataPartition(y = sampled_data$SALE.PRICE, p = 0.8, list = FALSE)
training_set <- sampled_data[partition_indexes, ]
testing_set <- sampled_data[-partition_indexes, ]
random_forest_model <- randomForest(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET,
data = training_set, ntree = 500)
predicted_values <- predict(random_forest_model, testing_set)
plot(testing_set$SALE.PRICE, predicted_values,
xlab = "Actual Sale Price",
ylab = "Predicted Sale Price",
main = "Comparison of Actual and Predicted Sale Prices")
set.seed(123)
selected_data <- clean.df %>% select(SALE.PRICE, GROSS.SQUARE.FEET, LAND.SQUARE.FEET)
sampled_data <- sample_n(selected_data, size = 5000)
partition_indexes <- createDataPartition(y = sampled_data$SALE.PRICE, p = 0.8, list = FALSE)
training_set <- sampled_data[partition_indexes, ]
testing_set <- sampled_data[-partition_indexes, ]
random_forest_model <- randomForest(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET,
data = training_set, ntree = 500)
predicted_values <- predict(random_forest_model, testing_set)
plot(testing_set$SALE.PRICE, predicted_values,
xlab = "Actual Sale Price",
ylab = "Predicted Sale Price",
main = "Comparison of Actual and Predicted Sale Prices")
# Setting a seed for reproducibility
set.seed(1024)
# Selecting the desired columns and sampling 7000 observations
sampled_data_set <- sample_n(data_3_cols, 7000)
# Setting a seed for reproducibility
set.seed(1024)
# Selecting the desired columns and sampling 7000 observations
sampled_data_set <- sample_n(selected_data, 7000)
# Creating training and testing indices
training_indices <- createDataPartition(y = sampled_data_set$SALE.PRICE, p = 0.8, list = FALSE)
# Splitting the data into training and testing sets
training_data <- sampled_data_set[training_indices, ]
testing_data <- sampled_data_set[-training_indices, ]
# Building a random forest model
random_forest_regressor <- randomForest(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET,
data = training_data,
ntree = 500)
# Making predictions on the test set
sale_price_predictions <- predict(random_forest_regressor, testing_data)
# Plotting the actual vs. predicted sale prices
plot(testing_data$SALE.PRICE, sale_price_predictions,
xlab = "Actual Sale Price",
ylab = "Predicted Sale Price",
main = "Actual vs. Predicted Sale Price for Sample of 7000 Observations")
set.seed(123)
sampled_data_set <- sample_n(selected_data, 7000)
training_indices <- createDataPartition(y = sampled_data_set$SALE.PRICE, p = 0.8, list = FALSE)
training_data <- sampled_data_set[training_indices, ]
testing_data <- sampled_data_set[-training_indices, ]
random_forest_regressor <- randomForest(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET,
data = training_data,
ntree = 500)
sale_price_predictions <- predict(random_forest_regressor, testing_data)
plot(testing_data$SALE.PRICE, sale_price_predictions,
xlab = "Actual Sale Price",
ylab = "Predicted Sale Price",
main = "Actual vs. Predicted Sale Price for Sample of 7000 Observations")
partition_indices_full_data <- createDataPartition(y = selected_data$SALE.PRICE, p = 0.8, list = FALSE)
training_set_full_data <- selected_data[partition_indices_full_data, ]
testing_set_full_data <- selected_data[-partition_indices_full_data, ]
random_forest_full_data <- randomForest(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET,
data = training_set_full_data,
ntree = 500)
predicted_prices_full_data <- predict(random_forest_full_data, testing_set_full_data)
plot(testing_set_full_data$SALE.PRICE, predicted_prices_full_data,
xlab = "Actual Sale Price",
ylab = "Predicted Sale Price",
main = "Actual vs. Predicted Sale Price - Full Data")
set.seed(42)
data1 = sample_n(clean_df_Without_Outliers, size = 1000)
model1 = lm(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET, data = data1)
summary(model1)
set.seed(43)
data2 = rbind(sample_n(clean_df_Without_Outliers, size = 1000),data1)
model2 = lm(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET, data = data2)
summary(model2)
set.seed(44)
data3 = rbind(sample_n(clean_df_Without_Outliers, size = 1000),data2)
model3 = lm(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET, data = data3)
summary(model3)
set.seed(42)
data1 = sample_n(clean_df_Without_Outliers, size = 1000)
model1 = lm(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET, data = data1)
summary(model1)
set.seed(43)
data2 = rbind(sample_n(clean_df_Without_Outliers, size = 1000),data1)
model2 = lm(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET, data = data2)
summary(model2)
set.seed(44)
data3 = rbind(sample_n(clean_df_Without_Outliers, size = 1000),data2)
model3 = lm(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET, data = data3)
summary(model3)
set.seed(123)
selected_data <- clean.df %>% select(SALE.PRICE, GROSS.SQUARE.FEET, LAND.SQUARE.FEET)
sampled_data <- sample_n(selected_data, size = 5000)
partition_indexes <- createDataPartition(y = sampled_data$SALE.PRICE, p = 0.8, list = FALSE)
training_set <- sampled_data[partition_indexes, ]
testing_set <- sampled_data[-partition_indexes, ]
random_forest_model <- randomForest(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET,
data = training_set, ntree = 500)
predicted_values <- predict(random_forest_model, testing_set)
plot(testing_set$SALE.PRICE, predicted_values,
xlab = "Actual Sale Price",
ylab = "Predicted Sale Price",
main = "Comparison of Actual and Predicted Sale Prices")
set.seed(123)
sampled_data_set <- sample_n(selected_data, 7000)
training_indices <- createDataPartition(y = sampled_data_set$SALE.PRICE, p = 0.8, list = FALSE)
training_data <- sampled_data_set[training_indices, ]
testing_data <- sampled_data_set[-training_indices, ]
random_forest_regressor <- randomForest(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET,
data = training_data,
ntree = 500)
sale_price_predictions <- predict(random_forest_regressor, testing_data)
plot(testing_data$SALE.PRICE, sale_price_predictions,
xlab = "Actual Sale Price",
ylab = "Predicted Sale Price",
main = "Actual vs. Predicted Sale Price for Sample of 7000 Observations")
partition_indices_full_data <- createDataPartition(y = selected_data$SALE.PRICE, p = 0.8, list = FALSE)
training_set_full_data <- selected_data[partition_indices_full_data, ]
testing_set_full_data <- selected_data[-partition_indices_full_data, ]
random_forest_full_data <- randomForest(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET,
data = training_set_full_data,
ntree = 500)
predicted_prices_full_data <- predict(random_forest_full_data, testing_set_full_data)
plot(testing_set_full_data$SALE.PRICE, predicted_prices_full_data,
xlab = "Actual Sale Price",
ylab = "Predicted Sale Price",
main = "Actual vs. Predicted Sale Price - Full Data")
cv_results <- train(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET, data=train_data, method="rf", trControl=trainControl(method="cv", number=5))
cv_results <- train(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET, data=training_set, method="rf", trControl=trainControl(method="cv", number=5))
cv_results
set.seed(123)
selected_data <- clean.df %>% select(SALE.PRICE, GROSS.SQUARE.FEET, LAND.SQUARE.FEET)
sampled_data <- sample_n(selected_data, size = 5000)
partition_indexes <- createDataPartition(y = sampled_data$SALE.PRICE, p = 0.8, list = FALSE)
training_set <- sampled_data[partition_indexes, ]
testing_set <- sampled_data[-partition_indexes, ]
random_forest_model <- randomForest(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET,
data = training_set, ntree = 500)
predicted_values <- predict(random_forest_model, testing_set)
plot(testing_set$SALE.PRICE, predicted_values,
xlab = "Actual Sale Price",
ylab = "Predicted Sale Price",
main = "Comparison of Actual and Predicted Sale Prices")
set.seed(123)
sampled_data_set2 <- sample_n(selected_data, 7000)
training_indices2 <- createDataPartition(y = sampled_data_set2$SALE.PRICE, p = 0.8, list = FALSE)
training_data2 <- sampled_data_set2[training_indices2, ]
testing_data2 <- sampled_data_set2[-training_indices2, ]
random_forest_regressor2 <- randomForest(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET,
data = training_data2,
ntree = 500)
sale_price_predictions2 <- predict(random_forest_regressor2, testing_data2)
plot(testing_data2$SALE.PRICE, sale_price_predictions2,
xlab = "Actual Sale Price",
ylab = "Predicted Sale Price",
main = "Actual vs. Predicted Sale Price for Sample of 7000 Observations")
cv_results <- train(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET, data=training_set, method="rf", trControl=trainControl(method="cv", number=5))
cv_results
cv_results <- train(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET, data=training_set, method="rf", trControl=trainControl(method="cv", number=5))
cv_results
cv_results2 <- train(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET, data=training_data2, method="rf", trControl=trainControl(method="cv", number=5))
cv_results2
cv_results3 <- train(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET, data=train_data_Full_data, method="rf", trControl=trainControl(method="cv", number=5))
cv_results <- train(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET, data=training_set, method="rf", trControl=trainControl(method="cv", number=5))
cv_results
cv_results2 <- train(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET, data=training_data2, method="rf", trControl=trainControl(method="cv", number=5))
cv_results2
cv_results3 <- train(SALE.PRICE ~ GROSS.SQUARE.FEET + LAND.SQUARE.FEET, data=training_set_full_data, method="rf", trControl=trainControl(method="cv", number=5))
cv_results3
library(ggplot2)
library(tidyverse)
library(readr)
library(readxl)
library(rpart)
library(dplyr)
library(randomForest)
library(class)
library(kknn)
library(caret)
library(stats)
data <- read.csv("C:/code/CSCI4600_hw/hw7/higher+education+students+performance+evaluation.csv")
data1 <- read_excel("C:/code/CSCI4600_hw/hw7/Absenteeism_at_work.xls")
summary(data)
hist(data$X1,
xlab = "x1",
ylab = "Frequency",
main = "Dostribution of x1",
col = "blue")
hist(data$COURSE.ID,
xlab = "COURSE ID",
ylab = "Frequency",
main = "Distribution of COURSE.ID",
col = "blue")
hist(data$GRADE,
xlab = "Grades",
ylab = "Frequency",
main = "Distribution of Grades",
col = "blue")
model <- lm(GRADE ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12 + X13 +
X14 + X15 + X16 + X17 + X18 + X19 + X20 + X21 + X22 + X23 + X24 + X25 +
X26 + X27 + X28 + X29 + X30, data = data)
summary(model) # Summarizes the model's performance
data_numeric <- data[sapply(data, is.numeric)]
data_scaled <- scale(data_numeric)
# Choose the number of clusters with the Elbow method
wss <- (nrow(data_scaled)-1)*sum(apply(data_scaled,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(data_scaled, centers=i)$withinss)
# Plot the Elbow curve
plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
# Perform K-means clustering with the chosen number of clusters
set.seed(123)
kmeans_result <- kmeans(data_scaled, centers=5)
print(kmeans_result)
data$cluster <- as.factor(kmeans_result$cluster)
summary(data1)
hist(data1$`Reason for absence`,
xlab = "Reason for absence",
ylab = "Frequency",
main = "Distribution of Reason for Absence",
col = "blue")
hist(data1$`Month of absence`,
xlab = "Month of absence",
ylab = "Frequency",
main = "Distribution of Month of Absence",
col = "blue")
hist(data1$`Day of the week`,
xlab = "Day of the week",
ylab = "Frequency",
main = "Distribution of Day of the week",
col = "blue")
hist(data1$Seasons,
xlab = "Seasons",
ylab = "Frequency",
main = "Distribution of Seasons",
col = "blue")
hist(data1$`Transportation expense`,
xlab = "Transportation expense",
ylab = "Frequency",
main = "Distribution of Transportation expense",
col = "blue")
hist(data1$`Distance from Residence to Work`,
xlab = "Distance from Residence to Work",
ylab = "Frequency",
main = "Distribution of Distance from Residence to Work",
col = "blue")
hist(data1$`Service time`,
xlab = "Service time",
ylab = "Frequency",
main = "Distribution of Service time",
col = "blue")
hist(data1$`Age`,
xlab = "Age",
ylab = "Frequency",
main = "Distribution of Age",
col = "blue")
model <- lm(`Absenteeism time in hours` ~ `Transportation expense` + `Distance from Residence to Work` + `Service time` + Age, data = data1)
summary(model)
data_numeric1 <- data1[sapply(data1, is.numeric)]
data_scaled1 <- scale(data_numeric1)
# Choose the number of clusters with the Elbow method
wss <- (nrow(data_scaled1)-1)*sum(apply(data_scaled1,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(data_scaled1, centers=i)$withinss)
# Plot the Elbow curve
plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
# Perform K-means clustering with the chosen number of clusters
set.seed(123)
kmeans_result <- kmeans(data_scaled1, centers=4)
print(kmeans_result)
data1$cluster <- as.factor(kmeans_result$cluster)
model <- lm(GRADE ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12 + X13 +
X14 + X15 + X16 + X17 + X18 + X19 + X20 + X21 + X22 + X23 + X24 + X25 +
X26 + X27 + X28 + X29 + X30, data = data)
summary(model)
set.seed(123)
dataSize <- nrow(data)
trainSize <- floor(0.7 * dataSize)
trainIndices <- sample(seq_len(dataSize), size = trainSize)
trainData <- data[trainIndices, ]
testData <- data[-trainIndices, ]
tree_model <- rpart(GRADE ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12 + X13 +
X14 + X15 + X16 + X17 + X18 + X19 + X20 + X21 + X22 + X23 + X24 + X25 +
X26 + X27 + X28 + X29 + X30, data = trainData, method = "class")
predictions <- predict(tree_model, testData, type = "class")
testData$GRADE <- factor(testData$GRADE)
confusionMatrix(predictions, testData$GRADE)
testData$GRADE <- as.numeric(testData$GRADE)
forest_model <- randomForest(GRADE ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12 + X13 +
X14 + X15 + X16 + X17 + X18 + X19 + X20 + X21 + X22 + X23 + X24 + X25 +
X26 + X27 + X28 + X29 + X30, data = data)
forest_model
rf_model <- randomForest(GRADE ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12 + X13 +
X14 + X15 + X16 + X17 + X18 + X19 + X20 + X21 + X22 + X23 + X24 + X25 +
X26 + X27 + X28 + X29 + X30, data = data, ntree = 500, mtry = 10)
oob_error <- rf_model$mse[rf_model$ntree]
print(oob_error)
data_numeric <- data[, sapply(data, is.numeric)]
data_scaled <- scale(data_numeric)
pca_result <- prcomp(data_scaled, center = TRUE, scale. = TRUE)
summary(pca_result)
plot(pca_result, type = "l")
linear_model <- lm(`Absenteeism time in hours` ~ ., data = data1)
summary(linear_model)
set.seed(123)
dataSize <- nrow(data1)
trainSize <- floor(0.7 * dataSize)
trainIndices <- sample(seq_len(dataSize), size = trainSize)
trainData <- data1[trainIndices, ]
testData <- data1[-trainIndices, ]
tree_model <- rpart(`Absenteeism time in hours` ~ `Transportation expense` + `Distance from Residence to Work` + `Service time` + Age, data = trainData, method = "class")
predictions <- predict(tree_model, testData, type = "class")
predictions <- as.factor(predictions)
predictions <- as.numeric(predictions)
errors <- predictions - testData$`Absenteeism time in hours`
mse <- mean(errors^2)
rmse <- sqrt(mse)
rmse
forest_model <- randomForest(`Absenteeism time in hours` ~ `Transportation expense` + `Distance from Residence to Work` + `Service time` + Age, data = data1)
library(ggplot2)
library(tidyverse)
library(readr)
library(readxl)
library(rpart)
library(dplyr)
library(randomForest)
library(class)
library(kknn)
library(caret)
library(stats)
data <- read.csv("C:/code/CSCI4600_hw/hw7/higher+education+students+performance+evaluation.csv")
data1 <- read_excel("C:/code/CSCI4600_hw/hw7/Absenteeism_at_work.xls")
summary(data)
hist(data$X1,
xlab = "x1",
ylab = "Frequency",
main = "Dostribution of x1",
col = "blue")
hist(data$COURSE.ID,
xlab = "COURSE ID",
ylab = "Frequency",
main = "Distribution of COURSE.ID",
col = "blue")
hist(data$GRADE,
xlab = "Grades",
ylab = "Frequency",
main = "Distribution of Grades",
col = "blue")
model <- lm(GRADE ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12 + X13 +
X14 + X15 + X16 + X17 + X18 + X19 + X20 + X21 + X22 + X23 + X24 + X25 +
X26 + X27 + X28 + X29 + X30, data = data)
summary(model) # Summarizes the model's performance
data_numeric <- data[sapply(data, is.numeric)]
data_scaled <- scale(data_numeric)
# Choose the number of clusters with the Elbow method
wss <- (nrow(data_scaled)-1)*sum(apply(data_scaled,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(data_scaled, centers=i)$withinss)
# Plot the Elbow curve
plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
# Perform K-means clustering with the chosen number of clusters
set.seed(123)
kmeans_result <- kmeans(data_scaled, centers=5)
print(kmeans_result)
data$cluster <- as.factor(kmeans_result$cluster)
summary(data1)
hist(data1$`Reason for absence`,
xlab = "Reason for absence",
ylab = "Frequency",
main = "Distribution of Reason for Absence",
col = "blue")
hist(data1$`Month of absence`,
xlab = "Month of absence",
ylab = "Frequency",
main = "Distribution of Month of Absence",
col = "blue")
hist(data1$`Day of the week`,
xlab = "Day of the week",
ylab = "Frequency",
main = "Distribution of Day of the week",
col = "blue")
hist(data1$Seasons,
xlab = "Seasons",
ylab = "Frequency",
main = "Distribution of Seasons",
col = "blue")
hist(data1$`Transportation expense`,
xlab = "Transportation expense",
ylab = "Frequency",
main = "Distribution of Transportation expense",
col = "blue")
hist(data1$`Distance from Residence to Work`,
xlab = "Distance from Residence to Work",
ylab = "Frequency",
main = "Distribution of Distance from Residence to Work",
col = "blue")
hist(data1$`Service time`,
xlab = "Service time",
ylab = "Frequency",
main = "Distribution of Service time",
col = "blue")
hist(data1$`Age`,
xlab = "Age",
ylab = "Frequency",
main = "Distribution of Age",
col = "blue")
model <- lm(`Absenteeism time in hours` ~ `Transportation expense` + `Distance from Residence to Work` + `Service time` + Age, data = data1)
summary(model)
data_numeric1 <- data1[sapply(data1, is.numeric)]
data_scaled1 <- scale(data_numeric1)
# Choose the number of clusters with the Elbow method
wss <- (nrow(data_scaled1)-1)*sum(apply(data_scaled1,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(data_scaled1, centers=i)$withinss)
# Plot the Elbow curve
plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
# Perform K-means clustering with the chosen number of clusters
set.seed(123)
kmeans_result <- kmeans(data_scaled1, centers=4)
print(kmeans_result)
data1$cluster <- as.factor(kmeans_result$cluster)
model <- lm(GRADE ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12 + X13 +
X14 + X15 + X16 + X17 + X18 + X19 + X20 + X21 + X22 + X23 + X24 + X25 +
X26 + X27 + X28 + X29 + X30, data = data)
summary(model)
set.seed(123)
dataSize <- nrow(data)
trainSize <- floor(0.7 * dataSize)
trainIndices <- sample(seq_len(dataSize), size = trainSize)
trainData <- data[trainIndices, ]
testData <- data[-trainIndices, ]
tree_model <- rpart(GRADE ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12 + X13 +
X14 + X15 + X16 + X17 + X18 + X19 + X20 + X21 + X22 + X23 + X24 + X25 +
X26 + X27 + X28 + X29 + X30, data = trainData, method = "class")
predictions <- predict(tree_model, testData, type = "class")
testData$GRADE <- factor(testData$GRADE)
confusionMatrix(predictions, testData$GRADE)
testData$GRADE <- as.numeric(testData$GRADE)
forest_model <- randomForest(GRADE ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12 + X13 +
X14 + X15 + X16 + X17 + X18 + X19 + X20 + X21 + X22 + X23 + X24 + X25 +
X26 + X27 + X28 + X29 + X30, data = data)
forest_model
rf_model <- randomForest(GRADE ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12 + X13 +
X14 + X15 + X16 + X17 + X18 + X19 + X20 + X21 + X22 + X23 + X24 + X25 +
X26 + X27 + X28 + X29 + X30, data = data, ntree = 500, mtry = 10)
oob_error <- rf_model$mse[rf_model$ntree]
print(oob_error)
data_numeric <- data[, sapply(data, is.numeric)]
data_scaled <- scale(data_numeric)
pca_result <- prcomp(data_scaled, center = TRUE, scale. = TRUE)
summary(pca_result)
plot(pca_result, type = "l")
linear_model <- lm(`Absenteeism time in hours` ~ ., data = data1)
summary(linear_model)
set.seed(123)
dataSize <- nrow(data1)
trainSize <- floor(0.7 * dataSize)
trainIndices <- sample(seq_len(dataSize), size = trainSize)
trainData <- data1[trainIndices, ]
testData <- data1[-trainIndices, ]
tree_model <- rpart(`Absenteeism time in hours` ~ `Transportation expense` + `Distance from Residence to Work` + `Service time` + Age, data = trainData, method = "class")
predictions <- predict(tree_model, testData, type = "class")
predictions <- as.factor(predictions)
predictions <- as.numeric(predictions)
errors <- predictions - testData$`Absenteeism time in hours`
mse <- mean(errors^2)
rmse <- sqrt(mse)
rmse
forest_model <- randomForest(`Absenteeism time in hours` ~ `Transportation expense` + `Distance from Residence to Work` + `Service time` + Age, data = data1)
forest_model <- randomForest(`Absenteeism time in hours` ~ ., data = data1)
forest_model <- randomForest(`Absenteeism time in hours` ~ `Distance from Residence to Work` + `Service time` + Age, data = data1)
forest_model <- randomForest(`Absenteeism time in hours` ~ data1$`Transportation expense` + `Distance from Residence to Work` + `Service time` + Age, data = data1)
forest_model <- randomForest(`Absenteeism time in hours` ~ data1$`Transportation expense` + `Distance from Residence to Work` + `Service time` + Age, data = data1)
forest_model <- randomForest(`Absenteeism time in hours` ~ data1$`Transportation expense` + data1$`Distance from Residence to Work` + data1$`Service time` + data1$Age, data = data1)
print(forest_model)
data_numeric <- data1[, sapply(data, is.numeric)]
data_numeric <- data1[, sapply(data1, is.numeric)]
data_scaled <- scale(data_numeric)
pca_result <- prcomp(data_scaled, center = TRUE, scale. = TRUE)
summary(pca_result)
plot(pca_result, type = "l")
