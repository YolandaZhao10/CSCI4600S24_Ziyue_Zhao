length(data)
shapiro.test(data)
data <- EPI_data$WATER_H
data <- as.numeric(data)
tf <- is.na(data)
data <- data[!tf]
length(data)
shapiro.test(data)
data <- EPI_data1$ENVHEALTH
data <- as.numeric(data)
tf <- is.na(data)
data <- data[!tf]
length(data)
shapiro.test(data)
data <- EPI_data1$DALY
data <- as.numeric(data)
tf <- is.na(data)
data <- data[!tf]
length(data)
shapiro.test(data)
data <- EPI_data1$AIR_H
data <- as.numeric(data)
tf <- is.na(data)
data <- data[!tf]
length(data)
shapiro.test(data)
data <- EPI_data1$WATER_H
data <- as.numeric(data)
tf <- is.na(data)
data <- data[!tf]
length(data)
shapiro.test(data)
# abalone dataset from UCI repository
# reading the dataset from UCI repository URL
abalone <- read.csv(url("https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data"), header = FALSE,
sep = ",")
# Column names
colnames(abalone) <- c("sex", "length", 'diameter', 'height', 'whole_weight', 'shucked_wieght', 'viscera_wieght', 'shell_weight',
'rings' )
# summary on abalone
summary(abalone)
# structure of the abalone data
str(abalone)
# summary of the abalone rings column
summary(abalone$rings)
# As shown above, the “rings” variable has a range between 1-29.
# This is the variable that we want to predict, and predicting this many levels
# might not give us the insight we’re looking for.
# For now, we’ll break the rings variable
# into 3 levels" “young” for abalones less than 8, “adult” for abalones between 8-11,
# and “old” for abalones older than 11.
abalone$rings <- as.numeric(abalone$rings)
abalone$rings <- cut(abalone$rings, br=c(-1,8,11,35), labels = c("young", 'adult', 'old'))
abalone$rings <- as.factor(abalone$rings)
summary(abalone$rings)
# remove the "sex" variable in abalone, because KNN requires all numeric variables for prediction
# z <- abalone
aba <- abalone
aba$sex <- NULL
# normalize the data using min max normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
aba[1:7] <- as.data.frame(lapply(aba[1:7], normalize))
summary(aba$shucked_wieght)
# After Normalization, each variable has a min of 0 and a max of 1.
# in other words, values are in the range from 0 to 1.
# We’ll now split the data into training and testing sets.
ind <- sample(2, nrow(aba), replace=TRUE, prob=c(0.7, 0.3))
KNNtrain <- aba[ind==1,]
KNNtest <- aba[ind==2,]
sqrt(2918)
# make k equal to the square root of 2918, the number of observations in the training set.
# sqrt(2918) ~= 54.01852 round it to 55 and use k = 55 # We usually take an Odd number for k value,
# knn model
# knn() is in the "class" library. Make sure to install it first on your RStudio.
library(class)
help("knn") # Read the knn documentation on RStudio.
KNNpred <- knn(train = KNNtrain[1:7], test = KNNtest[1:7], cl = KNNtrain$rings, k = 55)
KNNpred
table(KNNpred)
# iris dataset is from UCI ML repository.
library(ggplot2) # we will use ggplot2 to visualize the data.
head(iris) # first 6 rows of the
str(iris) # take a look at the structure of the iris data using str() function in R.
# dataset has 150 observations equally distributed observations among
# the three species: Setosa, Versicolor and Verginica.
summary(iris) # summary statistics of all the 4 variables Sepal.Length,Sepal.Width,
# Petal.Length and Petal.Width
help("sapply")
sapply(iris[,-5], var)
summary(iris)
# plot Sepal.Length Vs Sepal.Width using ggplot
ggplot(iris,aes(x = Sepal.Length, y = Sepal.Width, col= Species)) + geom_point()
# plot Petal.Length Vs Sepal.Width using ggplot
ggplot(iris,aes(x = Petal.Length, y = Petal.Width, col= Species)) + geom_point()
# kmeans clustering
# Read the documentation for kmeans() function
# https://stat.ethz.ch/R-manual/R-devel/library/stats/html/kmeans.html
set.seed(300)
k.max <- 12
# tot.withinss = Total within-cluster sum of square
# iter.max = the maximum number of iterations allowed
# nstart = if centers is a number, how many random sets should be chosen.
wss<- sapply(1:k.max,function(k){kmeans(iris[,3:4],k,nstart = 20,iter.max = 20)$tot.withinss})
wss # within sum of squares.
plot(1:k.max,wss, type= "b", xlab = "Number of clusters(k)", ylab = "Within cluster sum of squares")
icluster <- kmeans(iris[,3:4],3,nstart = 20)
table(icluster$cluster,iris$Species)
# In the table we can see that most of the observations have been clustered correctly
# In the table we can see that most of the observations have been clustered correctly
# however, 2 of the versicolor have been put in the cluster with all the virginica
# In the table we can see that most of the observations have been clustered correctly
# however, 2 of the versicolor have been put in the cluster with all the virginica
# and 4 of the verginica have been put in cluster 3 which mostly has versicolor.
# Classification ctrees
# iris data set
# Install the following libararies/packages
library(rpart)
library(rpart.plot)
# we will be using the iris dataset
iris
dim(iris) # check the dimensions of the iris dataset
# creating a sample from the iris dataset
s_iris <- sample(150,100)
s_iris
# creat testing and training sets
iris_train <-iris[s_iris,]
iris_test <-iris[-s_iris,]
dim(iris_test)
dim(iris_train)
# generate the decision tree model
dectionTreeModel <- rpart(Species~., iris_train, method = "class")
dectionTreeModel
#plotting the decision tree model using rpart.plot() function
rpart.plot(dectionTreeModel)
# In the table we can see that most of the observations have been clustered correctly
# however, 2 of the versicolor have been put in the cluster with all the virginica
# and 4 of the verginica have been put in cluster 3 which mostly has versicolor.
# Classification ctrees
# iris data set
# Install the following libararies/packages
library(rpart)
library(rpart.plot)
# we will be using the iris dataset
iris
dim(iris) # check the dimensions of the iris dataset
# creating a sample from the iris dataset
s_iris <- sample(150,100)
s_iris
# creat testing and training sets
iris_train <-iris[s_iris,]
iris_test <-iris[-s_iris,]
dim(iris_test)
dim(iris_train)
# generate the decision tree model
dectionTreeModel <- rpart(Species~., iris_train, method = "class")
dectionTreeModel
#plotting the decision tree model using rpart.plot() function
rpart.plot(dectionTreeModel)
# In the table we can see that most of the observations have been clustered correctly
# however, 2 of the versicolor have been put in the cluster with all the virginica
# and 4 of the verginica have been put in cluster 3 which mostly has versicolor.
# Classification ctrees
# iris data set
# Install the following libararies/packages
library(rpart)
library(rpart.plot)
install.packages(rpart.plot)
install.packages(rpart.plot)
install.packages('rpart.plot')
library(rpart.plot)
# we will be using the iris dataset
iris
dim(iris) # check the dimensions of the iris dataset
# creating a sample from the iris dataset
s_iris <- sample(150,100)
s_iris
# creat testing and training sets
iris_train <-iris[s_iris,]
iris_test <-iris[-s_iris,]
dim(iris_test)
dim(iris_train)
# generate the decision tree model
dectionTreeModel <- rpart(Species~., iris_train, method = "class")
dectionTreeModel
#plotting the decision tree model using rpart.plot() function
rpart.plot(dectionTreeModel)
# creating a matrix data with random numbers
# and plotting the matrix using the image() function
# you will see there, it does not have a real pattern in the plot.
set.seed(12345)
help(par)
# creating a matrix data with random numbers
# and plotting the matrix using the image() function
# you will see there, it does not have a real pattern in the plot.
set.seed(12345)
help(par)
# par can be used to set or query graphical parameters.
# Parameters can be set by specifying them as arguments
# to par in tag = value form, or by passing them as a list of tagged values.
par(mar = rep(0.2,4))
data_Matrix <-matrix(rnorm(400), nrow = 40)
image(1:10, 1:40, t(data_Matrix)[,nrow(data_Matrix):1])
# now we can run a hierarchical cluster analysis on the dataset
# we will use the heatmap() function that is available in R
help("heatmap") # read the documentation for
# the heatmap() function that is available in
#RStudio
#Read the documentation for rep()
help(rep)
par(mar=rep(0.2,4))
# the heatmap() function that is available in
#RStudio
#Read the documentation for rep()
help(rep)
par(mar=rep(0.2,4))
heatmap(data_Matrix)
# When we run the heatmap() here, we get the dendrograms printed on the both columns and
# the rows and still there is no real immerging pattern that is interesting to us,
# it is because there is no real interesting pattern underlying in the data we
# generated.
# Now we will add a pattern to the data by doing a random coin flip.
# we will use the rbinom() function along with a for-loop.
help("rbinom") # read the documentation for
set.seed(678910)
for(i in 1:40){
# flipping a coin and getting the data
coin_Flip <- rbinom(1, size = 1, prob = 0.5)
# if the coin is "Heads", add a common pattern to that row,
if(coin_Flip){
data_Matrix[i, ] <- data_Matrix[i, ] + rep(c(0,3), each =5)
}
}
# Now we will plot the data
# Now we can see that the right hand five columns have more yellow in them,
# which means they have a higher value and the left hand five columns that are little
# bit more in red color which means they have a lower value.
# it is because some of the rows have a mean of three in the right hand side, and
# some of the rows have mean of zero. Now we have introduced some pattern to it.
par(mar= rep(0.2, 4))
image(1:10, 1:40, t(data_Matrix)[, nrow(data_Matrix):1])
# Now we will plot the data
# Now we can see that the right hand five columns have more yellow in them,
# which means they have a higher value and the left hand five columns that are little
# bit more in red color which means they have a lower value.
# it is because some of the rows have a mean of three in the right hand side, and
# some of the rows have mean of zero. Now we have introduced some pattern to it.
par(mar= rep(0.2, 4))
image(1:10, 1:40, t(data_Matrix)[, nrow(data_Matrix):1])
set.seed(678910)
for(i in 1:40){
# flipping a coin and getting the data
coin_Flip <- rbinom(1, size = 1, prob = 0.5)
# if the coin is "Heads", add a common pattern to that row,
if(coin_Flip){
data_Matrix[i, ] <- data_Matrix[i, ] + rep(c(0,3), each =5)
}
}
# Now we will plot the data
# Now we can see that the right hand five columns have more yellow in them,
# which means they have a higher value and the left hand five columns that are little
# bit more in red color which means they have a lower value.
# it is because some of the rows have a mean of three in the right hand side, and
# some of the rows have mean of zero. Now we have introduced some pattern to it.
par(mar= rep(0.2, 4))
image(1:10, 1:40, t(data_Matrix)[, nrow(data_Matrix):1])
set.seed(678910)
for(i in 1:40){
# flipping a coin and getting the data
coin_Flip <- rbinom(1, size = 1, prob = 0.5)
# if the coin is "Heads", add a common pattern to that row,
if(coin_Flip){
data_Matrix[i, ] <- data_Matrix[i, ] + rep(c(0,3), each =5)
}
}
# Now we will plot the data
# Now we can see that the right hand five columns have more yellow in them,
# which means they have a higher value and the left hand five columns that are little
# bit more in red color which means they have a lower value.
# it is because some of the rows have a mean of three in the right hand side, and
# some of the rows have mean of zero. Now we have introduced some pattern to it.
par(mar= rep(0.2, 4))
image(1:10, 1:40, t(data_Matrix)[, nrow(data_Matrix):1])
set.seed(678910)
for(i in 1:40){
# flipping a coin and getting the data
coin_Flip <- rbinom(1, size = 1, prob = 0.5)
# if the coin is "Heads", add a common pattern to that row,
if(coin_Flip){
data_Matrix[i, ] <- data_Matrix[i, ] + rep(c(0,3), each =5)
}
}
# Now we will plot the data
# Now we can see that the right hand five columns have more yellow in them,
# which means they have a higher value and the left hand five columns that are little
# bit more in red color which means they have a lower value.
# it is because some of the rows have a mean of three in the right hand side, and
# some of the rows have mean of zero. Now we have introduced some pattern to it.
par(mar= rep(0.2, 4))
image(1:10, 1:40, t(data_Matrix)[, nrow(data_Matrix):1])
# creating a matrix data with random numbers
# and plotting the matrix using the image() function
# you will see there, it does not have a real pattern in the plot.
set.seed(12345)
help(par)
# par can be used to set or query graphical parameters.
# Parameters can be set by specifying them as arguments
# to par in tag = value form, or by passing them as a list of tagged values.
par(mar = rep(0.2,4))
data_Matrix <-matrix(rnorm(400), nrow = 40)
image(1:10, 1:40, t(data_Matrix)[,nrow(data_Matrix):1])
# now we can run a hierarchical cluster analysis on the dataset
# we will use the heatmap() function that is available in R
help("heatmap") # read the documentation for
# the heatmap() function that is available in
#RStudio
#Read the documentation for rep()
help(rep)
par(mar=rep(0.2,4))
heatmap(data_Matrix)
# When we run the heatmap() here, we get the dendrograms printed on the both columns and
# the rows and still there is no real immerging pattern that is interesting to us,
# it is because there is no real interesting pattern underlying in the data we
# generated.
# Now we will add a pattern to the data by doing a random coin flip.
# we will use the rbinom() function along with a for-loop.
help("rbinom") # read the documentation for the rbinom() function that
set.seed(678910)
for(i in 1:40){
# flipping a coin and getting the data
coin_Flip <- rbinom(1, size = 1, prob = 0.5)
# if the coin is "Heads", add a common pattern to that row,
if(coin_Flip){
data_Matrix[i, ] <- data_Matrix[i, ] + rep(c(0,3), each =5)
}
}
# Now we will plot the data
# Now we can see that the right hand five columns have more yellow in them,
# which means they have a higher value and the left hand five columns that are little
# bit more in red color which means they have a lower value.
# it is because some of the rows have a mean of three in the right hand side, and
# some of the rows have mean of zero. Now we have introduced some pattern to it.
par(mar= rep(0.2, 4))
image(1:10, 1:40, t(data_Matrix)[, nrow(data_Matrix):1])
# now we will run the heatmap() function on the data, we can see that, two
#sets of columns are easily separated.
par(mar=rep(0.2, 4))
heatmap(data_Matrix)
# Let's take a closer look at the patters in rows and columns by looking at the marginal
# means of the rows and columns.
# ten different columns mean and forty different rows means
hh <- hclust(dist(data_Matrix))
data_Matrix_Ordered <- data_Matrix[hh$order,]
par(mforw = c(1,3))
par(mfrow = c(1,3))
plot(rowMeans(data_Matrix_Ordered),40:1,xlab = "The Row Mean",ylab = "Row", pch = 19)
plot(rowMeans(data_Matrix_Ordered),xlab = "Column",ylab = "Column Mean",pch = 19)
# Exercise 2
abalone <- read.csv("c:/code/CSCI4600_hw/DA_Files/abalone.csv")
source("C:/code/CSCI4600_hw/Lab3/Lab3_part1.R", echo=TRUE)
View(abalone)
View(abalone)
plot(rowMeans(data_Matrix_Ordered),10:1,xlab = "Column",ylab = "Column Mean",pch = 19)
plot(colMeans(data_Matrix_Ordered),xlab = "Column",ylab = "Column Mean",pch = 19)
library(kknn)
install.packages(kknn)
install.packages(kknn)
install.packages("kknn")
# Exercise 2
abalone <- read.csv("c:/code/CSCI4600_hw/DA_Files/abalone.csv")
library(kknn)
spam.kknn <- kknn(spam~., train, test, distance = 1,
kernel = "triangular")
abalone <- read.csv("c:/code/CSCI4600_hw/DA_Files/abalone.csv")
naba<-dim(abalone)[1]
sampling.rate=0.7
num.test.set.labels=naba*(1.-sampling.rate)
training <-sample(1:naba,sampling.rate*naba, replace=FALSE)
train<-subset(abalone[training,],select=c("Sex","Length","Diameter","Height","Whole.weight","Shucked.weight","Viscera.weight","Shell.weight"))
testing<-setdiff(1:naba,training)
test<-subset(abalone[testing,],select=c("Sex","Length","Diameter","Height","Whole.weight","Shucked.weight","Viscera.weight","Shell.weight"))
crings<-abalone$Rings[training]
true.labels<-abalone$Rings[testing]
classif<-knn(train,test,crings,k=5)
??knn
?knn
# Exercise 2
library(kknn)
classif<-knn(train,test,crings,k=5)
classif<-kNN(train,test,crings,k=5)
install.packages(class)
install.packages(class)
library(class)
classif<-knn(train,test,crings,k=5)
classif
classif<-knn(train,test,crings,k=5)
train <- na.omit(train)
test <- na.omit(test)
classif<-knn(train,test,crings,k=5)
sampling.rate=0.9
num.test.set.labels=naba*(1.-sampling.rate)
training <-sample(1:naba,sampling.rate*naba, replace=FALSE)
train<-subset(abalone[training,],select=c("Sex","Length","Diameter","Height","Whole.weight","Shucked.weight","Viscera.weight","Shell.weight"))
testing<-setdiff(1:naba,training)
test<-subset(abalone[testing,],select=c("Sex","Length","Diameter","Height","Whole.weight","Shucked.weight","Viscera.weight","Shell.weight"))
crings<-abalone$Rings[training]
true.labels<-abalone$Rings[testing]
classif<-knn(train,test,crings,k=5)
summay(train)
summary(train)
View(train)
abalone <- as.numeric(ordered(abalone$Sex,levels = c("M","I","F")))
abalone <- read.csv("c:/code/CSCI4600_hw/DA_Files/abalone.csv")
abalone_sex <- as.numeric(ordered(abalone$Sex,levels = c("M","I","F")))
abalone$Sex[abalone$Sex == 'M'] <- '1'
abalone$Sex[abalone$Sex == 'I'] <- '2'
abalone$Sex[abalone$Sex == 'F'] <- '3'
abalone$Sex <- as.numeric(as.character(abalone$Sex))
naba<-dim(abalone)[1]
sampling.rate=0.9
num.test.set.labels=naba*(1.-sampling.rate)
training <-sample(1:naba,sampling.rate*naba, replace=FALSE)
train<-subset(abalone[training,],select=c("Sex","Length","Diameter","Height","Whole.weight","Shucked.weight","Viscera.weight","Shell.weight"))
testing<-setdiff(1:naba,training)
test<-subset(abalone[testing,],select=c("Sex","Length","Diameter","Height","Whole.weight","Shucked.weight","Viscera.weight","Shell.weight"))
crings<-abalone$Rings[training]
true.labels<-abalone$Rings[testing]
summary(train)
classif<-knn(train,test,crings,k=5)
classif
attributes(.Last.value)
spam.kknn <- kknn(spam~., train, test, distance = 1,
kernel = "triangular")
abalone.kknn <- kknn(abalone~., train, test, distance = 1,
kernel = "triangular")
attributes(.Last.value)
classif<-knn(train,test,crings,k=5)
classif
# Exercise 3
data(“iris”)
# Exercise 3
data("iris")
# Exercise 3
iris <- data("iris")
# Exercise 3
data("iris")
# Exercise 3
library(datasets)
data("iris")
summary(iris)
iris <- iris[-c(5)]
View(iris)
# Use map_dbl to run many models with varying value of k
sil_width <- map_dbl(2:10,  function(k){
model <- pam(x = scaled_data, k = k)
model$silinfo$avg.width
})
??mao_dbl
??map_dbl
library(tidyverse)
# Use map_dbl to run many models with varying value of k
sil_width <- map_dbl(2:10,  function(k){
model <- pam(x = scaled_data, k = k)
model$silinfo$avg.width
})
# Use map_dbl to run many models with varying value of k
sil_width <- map_dbl(2:10,  function(k){
model <- pam(x = scaled_data, k = k)
model$silinfo$avg.width
})
# Exercise 3
library(datasets)
data("iris")
summary(iris)
iris_new <- iris[-c(5)]
install.packages("stats")
install.packages("stats")
install.packages("stats")
library(stats)
data("iris")
summary(iris)
set.seed(123)
kmeans_result <- kmeans(iris_new,
centers = 3,
nstart = 25,
iter.max = 1000
)
print(kmeans_result$cluster)
print(kmeans_result$centers)
comparison_table <- table(iris_new[, 5], kmeans_result$cluster)
data("iris")
summary(iris)
View(iris)
iris$Species <- kmeans_result$cluster
summary(iris)
colnames(iris) <- c("Sepal.Length","Sepal.Width","Petal.Length","Petal.Width","cluster")
summary(iris)
View(iris_new)
library(gdata)
#faster xls reader but requires perl!
bronx1<-read.xls(file.choose(),pattern="BOROUGH",stringsAsFactors=FALSE,sheet=1,perl="<SOMEWHERE>/perl/bin/perl.exe")
library(gdata)
library(gdata)
#faster xls reader but requires perl!
bronx1<-read.xls(file.choose(),pattern="BOROUGH",stringsAsFactors=FALSE,sheet=1,perl="<SOMEWHERE>/perl/bin/perl.exe")
library(gdata)
#faster xls reader but requires perl!
bronx1<-read.xls(file.choose(),pattern="BOROUGH",stringsAsFactors=FALSE,sheet=1,perl="<SOMEWHERE>/perl/bin/perl.exe")
#faster xls reader but requires perl!
bronx1<-read.xls(file.choose(),pattern="BOROUGH",stringsAsFactors=FALSE,sheet=1,perl="<SOMEWHERE>/perl/bin/perl.exe")
bronx1<-bronx1[which(bronx1$GROSS.SQUARE.FEET!="0" & bronx1$LAND.SQUARE.FEET!="0" & bronx1$SALE.PRICE!="$0"),]
