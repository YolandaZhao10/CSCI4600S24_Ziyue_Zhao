fitSwiss <- randomForest(Fertility ~ Agriculture + Education + Catholic, data = swiss)
print(fitSwiss) # view results
importance(fitSwiss) # importance of each predictor
varImpPlot(fitSwiss)
plot(fitSwiss)
getTree(fitSwiss,1, labelVar=TRUE)
help(randomForest) # look at all the package contents and the randomForest method options
# look at rfcv - random forest cross-validation -
help(rfcv)
# other data....
data(imports85)
# perform randomForest and other tree methods.....
# perform randomForest and other tree methods.....
# perform randomForest and other tree methods.....
library(mlbench)
data(HouseVotes84)
model <- naiveBayes(Class ~ ., data = HouseVotes84)
nyt1<-read.csv("C:\code\CSCI4600_hw\DA_Files\nytimes\nyt1.csv")
nyt1<-read.csv("C:/code/CSCI4600_hw/DA_Files/nytimes/nyt1.csv")
nyt1<-nyt1[which(nyt1$Impressions>0 & nyt1$Clicks>0 & nyt1$Age>0),]
nnyt1<-dim(nyt1)[1]		# shrink it down!
sampling.rate=0.9
num.test.set.labels=nnyt1*(1.-sampling.rate)
training <-sample(1:nnyt1,sampling.rate*nnyt1, replace=FALSE)
train<-subset(nyt1[training,],select=c(Age,Impressions))
testing<-setdiff(1:nnyt1,training)
test<-subset(nyt1[testing,],select=c(Age,Impressions))
cg<-nyt1$Gender[training]
true.labels<-nyt1$Gender[testing]
classif<-knn(train,test,cg,k=5) #
?knn
library(igraph)
library(igraph)
classif<-knn(train,test,cg,k=5) #
?knn
library(kknn)
classif<-knn(train,test,cg,k=5) #
library(knn)
library(class)
library(class)
classif<-knn(train,test,cg,k=5) #
classif
attributes(.Last.value)
library(kknn)
data(iris)
m <- dim(iris)[1]
val <- sample(1:m, size = round(m/3), replace = FALSE, prob = rep(1/m, m))
iris.learn <- iris[-val,] 	# train
iris.valid <- iris[val,]	# test
iris.kknn <- train.kknn(Species~., iris.learn, distance = 1, kernel = c("triangular", "epanechnikov", "biweight", "triweight", "cos", "inv", "gaussian", "rank", "optimal") )
summary(iris.kknn)
table(predict(iris.kknn,iris.valid),iris.valid$Species)
head(iris.kknn$W)
head(iris.kknn$D)
head(iris.kknn$C)
View(iris.kknn)
iris.kknn$W
summary(iris.kknn)
library(kknn)
data(iris)
m <- dim(iris)[1]
val <- sample(1:m, size = round(m/3), replace = FALSE, prob = rep(1/m, m))
iris.learn <- iris[-val,] 	# train
iris.valid <- iris[val,]	# test
iris.kknn <- train.kknn(Species~., iris.learn, distance = 1, kernel = c("triangular", "epanechnikov", "biweight", "triweight", "cos", "inv", "gaussian", "rank", "optimal") )
summary(iris.kknn)
table(predict(iris.kknn,iris.valid),iris.valid$Species)
summary(iris.kknn)
head(iris.kknn$W)
head(iris.kknn$D)
head(iris.kknn$Sepal.Length)
head(iris.kknn$Sepal.Length)
summary(iris.kknn$Sepal.Length)
summary(iris.kknn)
summary(iris.kknn$data)
head(iris.kknn$data$Sepal.Length)
head(iris.kknn$data$Sepal.Width)
head(iris.kknn$data$W)
head(iris.kknn$data$Sepal.Length)
head(iris.kknn$data$Sepal.Width)
head(iris.kknn$data$Petal.Width)
head(iris.kknn$data$Petal.Length)
head(iris.kknn$data$Species)
head(iris.kknn$fittehead(iris.kknn$fittehead(iris.kknn$fitted.values)))
head(iris.kknn$fittehead)
head(iris.kknn$fitted.values)
library(kknn)
data(iris)
m <- dim(iris)[1]
val <- sample(1:m, size = round(m/3), replace = FALSE, prob = rep(1/m, m))
iris.learn <- iris[-val,] 	# train
iris.valid <- iris[val,]	# test
iris.kknn <- train.kknn(Species~., iris.learn, distance = 1, kernel = c("triangular", "epanechnikov", "biweight", "triweight", "cos", "inv", "gaussian", "rank", "optimal") )
summary(iris.kknn)
table(predict(iris.kknn,iris.valid),iris.valid$Species)
summary(iris.kknn$data)
head(iris.kknn$data$Sepal.Length)
head(iris.kknn$data$Sepal.Width)
head(iris.kknn$data$Petal.Width)
head(iris.kknn$data$Petal.Length)
head(iris.kknn$data$Species)
head(iris.kknn$fitted.values)
library(kknn)
data(iris)
m <- dim(iris)[1]
val <- sample(1:m, size = round(m/3), replace = FALSE,
prob = rep(1/m, m))
iris.learn <- iris[-val,]
iris.valid <- iris[val,]
iris.kknn <- kknn(Species~., iris.learn, iris.valid, distance = 1,
kernel = "triangular")
summary(iris.kknn)
fit <- fitted(iris.kknn)
table(iris.valid$Species, fit)
pcol <- as.character(as.numeric(iris.valid$Species))
pairs(iris.valid[1:4], pch = pcol, col = c("green3", "redâ€)[(iris.valid$Species != fit)+1])
P
)
))
library(e1071)
library(rpart)
data(Glass, package="mlbench")
library(mlbench)
data(Glass, package="mlbench")
index <- 1:nrow(Glass)
testindex <- sample(index, trunc(length(index)/3))
testset <- Glass[testindex,]
trainset <- Glass[-testindex,]
rpart.model <- rpart(Type ~ ., data = trainset)
rpart.pred <- predict(rpart.model, testset[,-10], type = "class")
printcp(rpart.model)
plotcp(rpart.model)
rsq.rpart(rpart.model)
print(rpart.model)
plot(rpart.model,compress=TRUE)
text(rpart.model, use.n=TRUE)
plot(rpart.pred)
# Conditional Inference Tree for Mileage
fit2M <- ctree(Mileage~Price + Country + Reliability + Type, data=na.omit(cu.summary))
summary(fit2M)
# plot tree
plot(fit2M, uniform=TRUE, main="CI Tree Tree for Mileage ")
text(fit2M, use.n=TRUE, all=TRUE, cex=.8)
# Conditional Inference Tree for Mileage
fit2M <- ctree(Mileage~Price + Country + Reliability + Type, data=na.omit(cu.summary))
summary(fit2M)
# plot tree
plot(fit2M, uniform=TRUE, main="CI Tree Tree for Mileage ")
text(fit2M, use.n=TRUE, all=TRUE, cex=.8)
# Conditional Inference Tree for Mileage
fit2M <- ctree(Mileage~Price + Country + Reliability + Type, data=na.omit(cu.summary))
summary(fit2M)
# plot tree
plot(fit2M, uniform=TRUE, main="CI Tree Tree for Mileage ")
text(fit2M, use.n=TRUE, all=TRUE, cex=.8)
pairs(~ Fertility + Education + Catholic, data = swiss, subset = Education < 20, main = "Swiss data, Education < 20")
require(party)
swiss_ctree <- ctree(Fertility ~ Agriculture + Education + Catholic, data = swiss)
plot(swiss_ctree)
library(party)
pairs(~ Fertility + Education + Catholic, data = swiss, subset = Education < 20, main = "Swiss data, Education < 20")
library(party)
swiss_ctree <- ctree(Fertility ~ Agriculture + Education + Catholic, data = swiss)
plot(swiss_ctree)
# Conditional Inference Tree for Mileage
fit2M <- ctree(Mileage~Price + Country + Reliability + Type, data=na.omit(cu.summary))
summary(fit2M)
# plot tree
plot(fit2M, uniform=TRUE, main="CI Tree Tree for Mileage ")
text(fit2M, use.n=TRUE, all=TRUE, cex=.8)
source("C:/code/CSCI4600_hw/DA_Files/group2/lab3_ctree2.R")
# Conditional Inference Tree for Mileage
class(fit2M)
fit2M <- ctree(Mileage~Price + Country + Reliability + Type, data=na.omit(cu.summary))
summary(fit2M)
# plot tree
plot(fit2M, uniform=TRUE, main="CI Tree Tree for Mileage ")
text(fit2M, use.n=TRUE, all=TRUE, cex=.8)
View(fit2M)
# plot tree
plot(fit2M, uniform=TRUE, main="CI Tree Tree for Mileage ")+
text(fit2M, use.n=TRUE, all=TRUE, cex=.8)
text(fit2M$data, use.n=TRUE, all=TRUE, cex=.8)
text(fit2M, use.n=TRUE, cex=.8)
library(rpart)
data(Glass, package="mlbench")
index <- 1:nrow(Glass)
testindex <- sample(index, trunc(length(index)/3))
testset <- Glass[testindex,]
trainset <- Glass[-testindex,]
rpart.model <- rpart(Type ~ ., data = trainset)
rpart.pred <- predict(rpart.model, testset[,-10], type = "class")
printcp(rpart.model)
plotcp(rpart.model)
rsq.rpart(rpart.model)
print(rpart.model)
plot(rpart.model,compress=TRUE)
text(rpart.model, use.n=TRUE)
plot(rpart.pred)
View(rpart.model)
plot(rpart.model,compress=TRUE)
# Conditional Inference Tree for Mileage
class(fit2M)
fit2M <- ctree(Mileage~Price + Country + Reliability + Type, data=na.omit(cu.summary))
summary(fit2M)
# plot tree
plot(fit2M, uniform=TRUE, main="CI Tree Tree for Mileage ")
text(fit2M, use.n=TRUE, all=TRUE, cex=.8)
# plot tree
plot(fit2M, uniform=TRUE, main="CI Tree Tree for Mileage ")
#text(fit2M, use.n=TRUE, all=TRUE, cex=.8)
text(fit2M, cex=.8)
#text(fit2M, use.n=TRUE, all=TRUE, cex=.8)
text(fit2M, cex=.8)
#text(fit2M, use.n=TRUE, all=TRUE, cex=.8)
plot(fit2M, tp_args = list(cex = 0.8))
# Conditional Inference Tree for Mileage
class(fit2M)
fit2M <- ctree(Mileage~Price + Country + Reliability + Type, data=na.omit(cu.summary))
summary(fit2M)
# plot tree
plot(fit2M, uniform=TRUE, main="CI Tree Tree for Mileage ")
#text(fit2M, use.n=TRUE, all=TRUE, cex=.8)
plot(fit2M, tp_args = list(cex = 0.8))
pairs(~ Fertility + Education + Catholic, data = swiss, subset = Education < 20, main = "Swiss data, Education < 20")
require(party)
swiss_ctree <- ctree(Fertility ~ Agriculture + Education + Catholic, data = swiss)
plot(swiss_ctree)
require(mlbench)
data(HouseVotes84)
library(klaR)
model <- NaiveBayes(Class ~ ., data = HouseVotes84)
predict(model, HouseVotes84[1:10,-1])
library(mlbench)
data(HouseVotes84)
library(klaR)
model <- NaiveBayes(Class ~ ., data = HouseVotes84)
predict(model, HouseVotes84[1:10,-1])
model <- NaiveBayes(Class ~ ., data = HouseVotes84)
predict(model, HouseVotes84[1:10,-1])
HouseVotes84 <- na.omit(HouseVotes84)
model <- NaiveBayes(Class ~ ., data = HouseVotes84)
predict(model, HouseVotes84[1:10,-1])
pred <- predict(model, HouseVotes84[,-1])
table(pred$class, HouseVotes84$Class)
fitK <- ctree(Kyphosis ~ Age + Number + Start, data=kyphosis)
plot(fitK, main="Conditional Inference Tree for Kyphosis")
plot(fitK, main="Conditional Inference Tree for Kyphosis",type="simple")
?plot
plot(fitK, main="Conditional Inference Tree for Kyphosis",type="simple")
plot(fitK, main="Conditional Inference Tree for Kyphosis",type="1")
plot(fitK, main="Conditional Inference Tree for Kyphosis",type="b")
plot(fitK, main="Conditional Inference Tree for Kyphosis",type="s")
plot(fitK, main="Conditional Inference Tree for Kyphosis",type="h")
plot(fitK, main="Conditional Inference Tree for Kyphosis",type="n")
?plot
plot(fitK, main="Conditional Inference Tree for Kyphosis")
plot(fitK, main="Conditional Inference Tree for Kyphosis",type="simple")
plot(fitK, main="Conditional Inference Tree for Kyphosis")
plot(fitK, main="Conditional Inference Tree for Kyphosis",type="simple",xlab = "Kyphosis Status")
plot(fitK, main="Conditional Inference Tree for Kyphosis",type="simple",xlab = "Kyphosis Status",ylab = "Proportion")
?plot
# Conditional Inference Tree for Mileage
fit2M <- ctree(Mileage~Price + Country + Reliability + Type, data=na.omit(cu.summary))
summary(fit2M)
# plot tree
plot(fit2M, uniform=TRUE, main="CI Tree Tree for Mileage ")
text(fit2M, use.n=TRUE, all=TRUE, cex=.8)
pairs(~ Fertility + Education + Catholic, data = swiss, subset = Education < 20, main = "Swiss data, Education < 20")
library(party)
swiss_ctree <- ctree(Fertility ~ Agriculture + Education + Catholic, data = swiss)
plot(swiss_ctree)
library(mlbench)
data(HouseVotes84)
library(klaR)
model <- NaiveBayes(Class ~ ., data = HouseVotes84)
predict(model, HouseVotes84[1:10,-1])
library(rpart)
Swiss_rpart <- rpart(Fertility ~ Agriculture + Education + Catholic, data = swiss)
plot(swiss_rpart) # try some different plot options
View(Swiss_rpart)
Swiss_rpart <- rpart(Fertility ~ Agriculture + Education + Catholic, data = swiss)
swiss_rpart <- rpart(Fertility ~ Agriculture + Education + Catholic, data = swiss)
plot(swiss_rpart) # try some different plot options
text(swiss_rpart) # try some different text options
data(swiss)
sclass <- kmeans(swiss[2:7], 3)
summary(swiss)
sclass <- kmeans(swiss[1:6], 3)
table(sclass$cluster, swiss[,2])
#
library(e1071)
m <- naiveBayes(swiss[2:7], swiss[,2])
m <- naiveBayes(swiss[1:6], swiss[,2])
table(predict(m, iswiss[1:6], swiss[,2]))
data(swiss)
summary(swiss)
sclass <- kmeans(swiss[1:6], 3)
table(sclass$cluster, swiss[,2])
#
library(e1071)
m <- naiveBayes(swiss[1:6], swiss[,2])
table(predict(m, iswiss[1:6], swiss[,2]))
?table
?predict
m <- naiveBayes(swiss[1:6], swiss[,2])
table(predict(m, swiss[1:6], swiss[,2]))
View(swiss)
m <- naiveBayes(swiss[,-2], swiss[,2])
table(predict(m, swiss[,-2], swiss[,2]))
table(predict(m, swiss[, -2]), swiss[, 2])
#
library(e1071)
m <- naiveBayes(swiss[, -2], swiss[, 2])
table(predict(m, swiss[, -2]), swiss[, 2])
#
library(e1071)
nam <- swiss[, -2]
m <- naiveBayes(nam, swiss[, 2])
table(predict(m, nam, swiss[, 2])
table(predict(m, nam, swiss[, 2]))
table(predict(m, nam, swiss[, 2]))
data(swiss)
sclass <- kmeans(swiss[2:6], 3)
table(sclass$cluster, swiss[,1])
library(car)
scatterplotMatrix(iris)
# and
scatterplotMatrix(swiss)
data(HairEyeColor)
mosaicplot(HairEyeColor)
margin.table(HairEyeColor,3)
margin.table(HairEyeColor,c(1,3))
data(swiss)
pairs(~ Fertility + Education + Catholic, data = swiss, subset = Education < 20, main = "Swiss data, Education < 20")
data(Titanic)
mdl <- naiveBayes(Survived ~ ., data = Titanic)
mdl
predict(mdl, Survived)
predict(mdl, Titanic)
data(Titanic)
mdl <- naiveBayes(Survived ~ ., data = Titanic)
mdl
summary(Titanic)
data(Titanic)
mdl <- naiveBayes(Survived ~ ., data = Titanic)
mdl
predict(mdl, Titanic)
num <- Titanic[,1]
num <- Titanic[1:10,1]
num <- Titanic[,1]
pred <- predict(model, Titanic[,-1])
table(pred$class, Titanic)
data(Titanic)
mdl <- naiveBayes(Survived ~ ., data = Titanic)
mdl
library(mlbench)
data(HouseVotes84)
library(klaR)
HouseVotes84 <- na.omit(HouseVotes84)
model <- NaiveBayes(Class ~ ., data = HouseVotes84)
predict(model, HouseVotes84[1:10,-1])
pred <- predict(model, HouseVotes84[,-1])
table(pred$class, HouseVotes84$Class)
# Josh Walters
#install.packages('ElemStatLearn')
library(ElemStatLearn)
library(klaR) # different from e1071 naivebayes - try it too!
library(caret)
# Josh Walters
#install.packages('ElemStatLearn')
library(ElemStatLearn)
library(klaR) # different from e1071 naivebayes - try it too!
library(caret)
data(spam, package="ElemStatLearn")
sub = sample(nrow(spam), floor(nrow(spam) * 0.9))
train = spam[sub,]
test = spam[-sub,]
xTrain = train[,-58]
yTrain = train$spam
xTest = test[,-58]
yTest = test$spam
model = train(xTrain,yTrain,'nb',trControl=trainControl(method='cv',number=10))
prop.table(table(predict(model$finalModel,xTest)$class,yTest))
# Alternate way to set up a training sample
train.ind <- sample(1:nrow(spam), ceiling(nrow(spam)*2/3), replace=FALSE)
# apply NB classifier
nb.res <- NaiveBayes(spam ~ ., data=spam[train.ind,])
# predict on holdout units
nb.pred <- predict(nb.res, spam[-train.ind,])
# but this also works on the training sample, i.e. without using a `newdata`
head(predict(nb.res))
# Josh Walters
#install.packages('ElemStatLearn')
library(ElemStatLearn)
library(klaR) # different from e1071 naivebayes - try it too!
library(caret)
data(spam, package="ElemStatLearn")
spam <- na.omit(spam)
sub = sample(nrow(spam), floor(nrow(spam) * 0.9))
train = spam[sub,]
test = spam[-sub,]
xTrain = train[,-58]
yTrain = train$spam
xTest = test[,-58]
yTest = test$spam
model = train(xTrain,yTrain,'nb',trControl=trainControl(method='cv',number=10))
prop.table(table(predict(model$finalModel,xTest)$class,yTest))
# Alternate way to set up a training sample
train.ind <- sample(1:nrow(spam), ceiling(nrow(spam)*2/3), replace=FALSE)
# apply NB classifier
nb.res <- NaiveBayes(spam ~ ., data=spam[train.ind,])
# predict on holdout units
nb.pred <- predict(nb.res, spam[-train.ind,])
# but this also works on the training sample, i.e. without using a `newdata`
head(predict(nb.res))
library(kknn)
spam.kknn <- kknn(spam~., train, test, distance = 1,
kernel = "triangular")
summary(spam.kknn)
data(swiss)
summary(swiss)
sclass <- kmeans(swiss[1:6], 3)
table(sclass$cluster, swiss[,2])
#
library(e1071)
nam <- swiss[, -2]
m <- naiveBayes(nam, swiss[, 2])
table(predict(m, nam, swiss[, 2]))
m <- naiveBayes(swiss[1:6], swiss[,-2])
data(swiss)
summary(swiss)
sclass <- kmeans(swiss[1:6], 3)
View(sclass)
table(sclass$cluster, swiss[,2])
#nb.res <- NaiveBayes(spam ~ ., data=spam[train.ind,])
m <- NaiveBayes(swiss, swiss[,-2])
b <- swiss[, 1]
a <- swiss[, -1]
m <- NaiveBayes(a, b)
data(swiss)
summary(swiss)
sclass <- kmeans(swiss[1:6], 3)
table(sclass$cluster, swiss[,2])
#
library(e1071)
b <- swiss[, 1]
a <- swiss[, -1]
m <- NaiveBayes(a, b)
table(predict(m, a, b))
m <- naiveBayes(sclass$cluster, swiss[,2])
table(predict(m, sclass$cluster, swiss[,2]))
m <- naiveBayes(sclass$cluster, swiss[,2])
m <- naiveBayes(swiss[1:6], swiss[,2])
table(predict(m, iswiss[1:6], swiss[,2]))
data(swiss)
summary(swiss)
sclass <- kmeans(swiss[1:6], 3)
table(sclass$cluster, swiss[,2])
#
library(e1071)
data(swiss)
summary(swiss)
sclass <- kmeans(swiss[1:6], 3)
table(sclass$cluster, swiss[,2])
#
library(e1071)
m <- naiveBayes(swiss[1:6], swiss[,2])
table(predict(m, iswiss[1:6], swiss[,2]))
# Josh Walters
#install.packages('ElemStatLearn')
library(ElemStatLearn)
library(klaR) # different from e1071 naivebayes - try it too!
library(caret)
data(spam, package="ElemStatLearn")
sub = sample(nrow(spam), floor(nrow(spam) * 0.9))
train = spam[sub,]
test = spam[-sub,]
xTrain = train[,-58]
yTrain = train$spam
xTest = test[,-58]
yTest = test$spam
model = train(xTrain,yTrain,'nb',trControl=trainControl(method='cv',number=10))
prop.table(table(predict(model$finalModel,xTest)$class,yTest))
# Alternate way to set up a training sample
train.ind <- sample(1:nrow(spam), ceiling(nrow(spam)*2/3), replace=FALSE)
# apply NB classifier
nb.res <- NaiveBayes(spam ~ ., data=spam[train.ind,])
# predict on holdout units
nb.pred <- predict(nb.res, spam[-train.ind,])
# but this also works on the training sample, i.e. without using a `newdata`
head(predict(nb.res))
data(swiss)
summary(swiss)
sclass <- kmeans(swiss[1:6], 3)
table(sclass$cluster, swiss[,2])
#
library(e1071)
m <- naiveBayes(swiss[1:6], swiss[,2])
View(sclass)
View(m)
#
library(e1071)
m <- naiveBayes(swiss[1:6], swiss[,2])
table(predict(m, iswiss[1:6], swiss[,2]))
data(swiss)
summary(swiss)
sclass <- kmeans(swiss[1:6], 3)
table(sclass$cluster, swiss[,2])
#
library(e1071)
m <- naiveBayes(swiss[1:6], swiss[,2])
